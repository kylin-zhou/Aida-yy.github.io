<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Deep learning on Aida’s home</title>
    <link>https://aida-yy.github.io/categories/deep-learning/</link>
    <description>Recent content in Deep learning on Aida’s home</description>
    <image>
      <url>https://aida-yy.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://aida-yy.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sun, 21 Aug 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://aida-yy.github.io/categories/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>HOW POWERFUL GNN</title>
      <link>https://aida-yy.github.io/posts/gnn_powerful_gin/</link>
      <pubDate>Sun, 21 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>https://aida-yy.github.io/posts/gnn_powerful_gin/</guid>
      <description>HOW POWERFUL ARE GRAPH NEURAL NETWORKS?
本文是 Jure Leskovec 又一力作，首先对图神经网络的原理做了深入检出、提纲挈领的叙述，然后从原理方面介绍了如何发挥图神经网络的效用。
图神经网络可以分为三个阶段：
Aggregate：聚合邻居节点信息 $$ a^{(k)}{v} = AGGREGATE^{(k)}({h{\mu}^{(k−1)}:\mu\in N(v)}) $$
Combine：聚合邻居和当前节点 $$ h_{\mu}^{(k)} = COMBINE^{(k)}({h_{\mu}^{(k−1)},a^{(k)}_v}) $$
Readout：整合表示图中所有节点 $$ h_G = READOUT({h^{(K)}_v|v \in G}) $$
在GraphSAGE中，Aggregate和Combine过程如下，GCN同理
那么如何衡量图神经网络是否学到了良好的表示，这里提到了 Weisfeiler-Lehman test ，有兴趣可以下去研究。
文章的和核心出发点在于：对于子树结构相同且对应节点特征相同的的俩个节点，一个有效的GNN应该有能力映射两个节点到embedding空间中相同的位置，决不会将两个不同的节点映射到同一embedding空间位置。
Intuitively, a maximally powerful GNN maps two nodes to the same location only if they have identical subtree structures with identical features on the corresponding nodes
A maximally powerful GNN would never map two different neighborhoods</description>
    </item>
    
    <item>
      <title>并行化tf数据生成器</title>
      <link>https://aida-yy.github.io/posts/%E5%B9%B6%E8%A1%8C%E5%8C%96tf%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90%E5%99%A8/</link>
      <pubDate>Sat, 20 Aug 2022 11:30:03 +0000</pubDate>
      
      <guid>https://aida-yy.github.io/posts/%E5%B9%B6%E8%A1%8C%E5%8C%96tf%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90%E5%99%A8/</guid>
      <description>在处理大规模数据时，数据无法全部载入内存，我们通常用两个选项
使用tfrecords 使用 tf.data.Dataset.from_generator() tfrecords的并行化使用前文已经有过介绍，这里不再赘述。如果我们不想生成tfrecord中间文件，那么生成器就是你所需要的。
本文主要记录针对 from_generator()的并行化方法，在 tf.data 中，并行化主要通过 map和 num_parallel_calls 实现，但是对一些场景，我们的generator()中有一些处理逻辑，是无法直接并行化的，最简单的方法就是将generator()中的逻辑抽出来，使用map实现。
tf.data.Dataset generator 并行 对generator()中的复杂逻辑，我们对其进行简化，即仅在生成器中做一些下标取值的类型操作，将generator()中处理部分使用py_function 包裹(wrapped) ，然后调用map处理。
def func(i): i = i.numpy() # Decoding from the EagerTensor object x, y = your_processing_function(training_set[i]) return x, y z = list(range(len(training_set))) # The index generator dataset = tf.data.Dataset.from_generator(lambda: z, tf.uint8) dataset = dataset.map(lambda i: tf.py_function(func=func, inp=[i], Tout=[tf.uint8, tf.float32] ), num_parallel_calls=tf.data.AUTOTUNE) 由于隐式推断的原因，有时tensor的输出shape是未知的，需要额外处理
A Tensor&amp;rsquo;s shape (that is, the rank of the Tensor and the size of each dimension) may not always be fully known.</description>
    </item>
    
    <item>
      <title>模型召回之DSSM</title>
      <link>https://aida-yy.github.io/posts/%E6%A8%A1%E5%9E%8B%E5%8F%AC%E5%9B%9E%E4%B9%8Bdssm/</link>
      <pubDate>Sat, 20 Aug 2022 11:30:03 +0000</pubDate>
      
      <guid>https://aida-yy.github.io/posts/%E6%A8%A1%E5%9E%8B%E5%8F%AC%E5%9B%9E%E4%B9%8Bdssm/</guid>
      <description>模型召回之DSSM 双塔模型
负样本构造：训练前构造或训练时批内构造
实现 model from transformers import AutoConfig,AutoTokenizer,TFAutoModel MODEL_NAME = &amp;#34;hfl/chinese-roberta-wwm-ext&amp;#34; tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) config = AutoConfig.from_pretrained(MODEL_NAME) # backbone = TFAutoModel.from_pretrained(MODEL_NAME) # tokenizer.save_pretrained(&amp;#39;model&amp;#39;) # config.save_pretrained(&amp;#39;model&amp;#39;) # backbone.save_pretrained(&amp;#39;model&amp;#39;) class baseModel(tf.keras.Model): def __init__(self,MODEL_NAME,finetune=False,pooler=&amp;#34;avg&amp;#34;): super().__init__() self.pooler = pooler self.backbone = TFAutoModel.from_pretrained(MODEL_NAME) if not finetune: self.backbone.trainable = False print(&amp;#34;bert close&amp;#34;) self.dense_layer = tf.keras.layers.Dense(128) def call(self,inputs): x = self.backbone(inputs) if self.pooler == &amp;#34;cls&amp;#34;: x = x[1] elif self.pooler == &amp;#34;avg&amp;#34;: x = tf.reduce_mean(x[0],axis=1) elif self.</description>
    </item>
    
    <item>
      <title>模型召回之SimCSE</title>
      <link>https://aida-yy.github.io/posts/%E6%A8%A1%E5%9E%8B%E5%8F%AC%E5%9B%9E%E4%B9%8Bsimcse/</link>
      <pubDate>Sat, 20 Aug 2022 11:30:03 +0000</pubDate>
      
      <guid>https://aida-yy.github.io/posts/%E6%A8%A1%E5%9E%8B%E5%8F%AC%E5%9B%9E%E4%B9%8Bsimcse/</guid>
      <description>模型召回之SimCSE
[TOC]
dataset unsuper import numpy as np import math class UnsuperviseData(tf.keras.utils.Sequence): def __init__(self, x_set, batch_size): self.x = x_set self.batch_size = batch_size def __len__(self): return math.ceil(len(self.x) / self.batch_size) def __getitem__(self, idx): batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size] batch_x = batch_x + batch_x bx = np.array([batch_x[i::self.batch_size] for i in range(self.batch_size)]).flatten().tolist() return self._tokenizer(bx) def _tokenizer(self,x): return tokenizer(x, max_length=50, padding=True,truncation=True,return_tensors=&amp;#34;tf&amp;#34;) super class SuperviseData(tf.keras.utils.Sequence): def __init__(self, query_set, doc_set, corpus, batch_size): self.querys = query_set self.</description>
    </item>
    
  </channel>
</rss>
