<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>并行化tf数据生成器 | Aida’s home</title><meta name=keywords content="tensorflow"><meta name=description content="在处理大规模数据时，数据无法全部载入内存，我们通常用两个选项
使用tfrecords 使用 tf.data.Dataset.from_generator() tfrecords的并行化使用前文已经有过介绍，这里不再赘述。如果我们不想生成tfrecord中间文件，那么生成器就是你所需要的。
本文主要记录针对 from_generator()的并行化方法，在 tf.data 中，并行化主要通过 map和 num_parallel_calls 实现，但是对一些场景，我们的generator()中有一些处理逻辑，是无法直接并行化的，最简单的方法就是将generator()中的逻辑抽出来，使用map实现。
tf.data.Dataset generator 并行 对generator()中的复杂逻辑，我们对其进行简化，即仅在生成器中做一些下标取值的类型操作，将generator()中处理部分使用py_function 包裹(wrapped) ，然后调用map处理。
def func(i): i = i.numpy() # Decoding from the EagerTensor object x, y = your_processing_function(training_set[i]) return x, y z = list(range(len(training_set))) # The index generator dataset = tf.data.Dataset.from_generator(lambda: z, tf.uint8) dataset = dataset.map(lambda i: tf.py_function(func=func, inp=[i], Tout=[tf.uint8, tf.float32] ), num_parallel_calls=tf.data.AUTOTUNE) 由于隐式推断的原因，有时tensor的输出shape是未知的，需要额外处理
A Tensor&rsquo;s shape (that is, the rank of the Tensor and the size of each dimension) may not always be fully known."><meta name=author content="Me"><link rel=canonical href=https://aida-yy.github.io/posts/%E5%B9%B6%E8%A1%8C%E5%8C%96tf%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90%E5%99%A8/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.3299c596a7007118365635c056dd427dace22b7b8c1341fdef6fa6c31359ba10.css integrity rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.acb54fd32bbc1982428b8850317e45d076b95012730a5936667e6bc21777692a.js integrity onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://aida-yy.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://aida-yy.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://aida-yy.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://aida-yy.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://aida-yy.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="并行化tf数据生成器"><meta property="og:description" content="在处理大规模数据时，数据无法全部载入内存，我们通常用两个选项
使用tfrecords 使用 tf.data.Dataset.from_generator() tfrecords的并行化使用前文已经有过介绍，这里不再赘述。如果我们不想生成tfrecord中间文件，那么生成器就是你所需要的。
本文主要记录针对 from_generator()的并行化方法，在 tf.data 中，并行化主要通过 map和 num_parallel_calls 实现，但是对一些场景，我们的generator()中有一些处理逻辑，是无法直接并行化的，最简单的方法就是将generator()中的逻辑抽出来，使用map实现。
tf.data.Dataset generator 并行 对generator()中的复杂逻辑，我们对其进行简化，即仅在生成器中做一些下标取值的类型操作，将generator()中处理部分使用py_function 包裹(wrapped) ，然后调用map处理。
def func(i): i = i.numpy() # Decoding from the EagerTensor object x, y = your_processing_function(training_set[i]) return x, y z = list(range(len(training_set))) # The index generator dataset = tf.data.Dataset.from_generator(lambda: z, tf.uint8) dataset = dataset.map(lambda i: tf.py_function(func=func, inp=[i], Tout=[tf.uint8, tf.float32] ), num_parallel_calls=tf.data.AUTOTUNE) 由于隐式推断的原因，有时tensor的输出shape是未知的，需要额外处理
A Tensor&rsquo;s shape (that is, the rank of the Tensor and the size of each dimension) may not always be fully known."><meta property="og:type" content="article"><meta property="og:url" content="https://aida-yy.github.io/posts/%E5%B9%B6%E8%A1%8C%E5%8C%96tf%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90%E5%99%A8/"><meta property="og:image" content="https://aida-yy.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-08-20T11:30:03+00:00"><meta property="article:modified_time" content="2022-08-20T11:30:03+00:00"><meta property="og:site_name" content="Aida’s home"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://aida-yy.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="并行化tf数据生成器"><meta name=twitter:description content="在处理大规模数据时，数据无法全部载入内存，我们通常用两个选项
使用tfrecords 使用 tf.data.Dataset.from_generator() tfrecords的并行化使用前文已经有过介绍，这里不再赘述。如果我们不想生成tfrecord中间文件，那么生成器就是你所需要的。
本文主要记录针对 from_generator()的并行化方法，在 tf.data 中，并行化主要通过 map和 num_parallel_calls 实现，但是对一些场景，我们的generator()中有一些处理逻辑，是无法直接并行化的，最简单的方法就是将generator()中的逻辑抽出来，使用map实现。
tf.data.Dataset generator 并行 对generator()中的复杂逻辑，我们对其进行简化，即仅在生成器中做一些下标取值的类型操作，将generator()中处理部分使用py_function 包裹(wrapped) ，然后调用map处理。
def func(i): i = i.numpy() # Decoding from the EagerTensor object x, y = your_processing_function(training_set[i]) return x, y z = list(range(len(training_set))) # The index generator dataset = tf.data.Dataset.from_generator(lambda: z, tf.uint8) dataset = dataset.map(lambda i: tf.py_function(func=func, inp=[i], Tout=[tf.uint8, tf.float32] ), num_parallel_calls=tf.data.AUTOTUNE) 由于隐式推断的原因，有时tensor的输出shape是未知的，需要额外处理
A Tensor&rsquo;s shape (that is, the rank of the Tensor and the size of each dimension) may not always be fully known."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://aida-yy.github.io/posts/"},{"@type":"ListItem","position":2,"name":"并行化tf数据生成器","item":"https://aida-yy.github.io/posts/%E5%B9%B6%E8%A1%8C%E5%8C%96tf%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90%E5%99%A8/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"并行化tf数据生成器","name":"并行化tf数据生成器","description":"在处理大规模数据时，数据无法全部载入内存，我们通常用两个选项\n使用tfrecords 使用 tf.data.Dataset.from_generator() tfrecords的并行化使用前文已经有过介绍，这里不再赘述。如果我们不想生成tfrecord中间文件，那么生成器就是你所需要的。\n本文主要记录针对 from_generator()的并行化方法，在 tf.data 中，并行化主要通过 map和 num_parallel_calls 实现，但是对一些场景，我们的generator()中有一些处理逻辑，是无法直接并行化的，最简单的方法就是将generator()中的逻辑抽出来，使用map实现。\ntf.data.Dataset generator 并行 对generator()中的复杂逻辑，我们对其进行简化，即仅在生成器中做一些下标取值的类型操作，将generator()中处理部分使用py_function 包裹(wrapped) ，然后调用map处理。\ndef func(i): i = i.numpy() # Decoding from the EagerTensor object x, y = your_processing_function(training_set[i]) return x, y z = list(range(len(training_set))) # The index generator dataset = tf.data.Dataset.from_generator(lambda: z, tf.uint8) dataset = dataset.map(lambda i: tf.py_function(func=func, inp=[i], Tout=[tf.uint8, tf.float32] ), num_parallel_calls=tf.data.AUTOTUNE) 由于隐式推断的原因，有时tensor的输出shape是未知的，需要额外处理\nA Tensor\u0026rsquo;s shape (that is, the rank of the Tensor and the size of each dimension) may not always be fully known.","keywords":["tensorflow"],"articleBody":"在处理大规模数据时，数据无法全部载入内存，我们通常用两个选项\n使用tfrecords 使用 tf.data.Dataset.from_generator() tfrecords的并行化使用前文已经有过介绍，这里不再赘述。如果我们不想生成tfrecord中间文件，那么生成器就是你所需要的。\n本文主要记录针对 from_generator()的并行化方法，在 tf.data 中，并行化主要通过 map和 num_parallel_calls 实现，但是对一些场景，我们的generator()中有一些处理逻辑，是无法直接并行化的，最简单的方法就是将generator()中的逻辑抽出来，使用map实现。\ntf.data.Dataset generator 并行 对generator()中的复杂逻辑，我们对其进行简化，即仅在生成器中做一些下标取值的类型操作，将generator()中处理部分使用py_function 包裹(wrapped) ，然后调用map处理。\ndef func(i): i = i.numpy() # Decoding from the EagerTensor object x, y = your_processing_function(training_set[i]) return x, y z = list(range(len(training_set))) # The index generator dataset = tf.data.Dataset.from_generator(lambda: z, tf.uint8) dataset = dataset.map(lambda i: tf.py_function(func=func, inp=[i], Tout=[tf.uint8, tf.float32] ), num_parallel_calls=tf.data.AUTOTUNE) 由于隐式推断的原因，有时tensor的输出shape是未知的，需要额外处理\nA Tensor’s shape (that is, the rank of the Tensor and the size of each dimension) may not always be fully known. In tf.function definitions, the shape may only be partially known.\nMost operations produce tensors of fully-known shapes if the shapes of their inputs are also fully known, but in some cases it’s only possible to find the shape of a tensor at execution time.\ndataset = dataset.batch(8)\rdef _fixup_shape(x, y):\rx.set_shape([None, None, None, nb_channels]) # n, h, w, c\ry.set_shape([None, nb_classes]) # n, nb_classes\rreturn x, y\rdataset = dataset.map(_fixup_shape) tf.Tensor与tf.EagerTensor 为什么需要 tf.py_function，先来看下tf.Tensor与tf.EagerTensor\nEagerTensor是实时的，可以在任何时候获取到它的值，即通过numpy获取\nTensor是非实时的，它是静态图中的组件，只有当喂入数据、运算完成才能获得该Tensor的值，\nmap中映射的函数运算，而仅仅是告诉dataset，你每一次拿出来的样本时要先进行一遍function运算之后才使用的，所以function的调用是在每次迭代dataset的时候才调用的，属于静态图逻辑\ntensorflow.python.framework.ops.EagerTensor\rtensorflow.python.framework.ops.Tensor tf.py_function在这里起了什么作用？\nWraps a python function into a TensorFlow op that executes it eagerly.\n刚才说到map数据静态图逻辑，默认参数都是Tensor。而 使用tf.py_function()包装后，参数就变成了EagerTensor。\nreferences 【1】https://medium.com/@acordier/tf-data-dataset-generators-with-parallelization-the-easy-way-b5c5f7d2a18\n【2】https://blog.csdn.net/qq_27825451/article/details/105247211\n【3】https://www.tensorflow.org/guide/data_performance#parallelizing_data_extraction\n","wordCount":"178","inLanguage":"en","datePublished":"2022-08-20T11:30:03Z","dateModified":"2022-08-20T11:30:03Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://aida-yy.github.io/posts/%E5%B9%B6%E8%A1%8C%E5%8C%96tf%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90%E5%99%A8/"},"publisher":{"@type":"Organization","name":"Aida’s home","logo":{"@type":"ImageObject","url":"https://aida-yy.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://aida-yy.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://aida-yy.github.io/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://aida-yy.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://aida-yy.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://aida-yy.github.io/tags/ title=tags><span>tags</span></a></li><li><a href=https://aida-yy.github.io/booklist/ title=书单><span>书单</span></a></li><li><a href=https://aida-yy.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://aida-yy.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://aida-yy.github.io/posts/>Posts</a></div><h1 class=post-title>并行化tf数据生成器</h1><div class=post-meta><span title='2022-08-20 11:30:03 +0000 +0000'>August 20, 202020</span>&nbsp;·&nbsp;1 min&nbsp;·&nbsp;178 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/%e5%b9%b6%e8%a1%8c%e5%8c%96tf%e6%95%b0%e6%8d%ae%e7%94%9f%e6%88%90%e5%99%a8.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><ul><li><a href=#references>references</a></li></ul></li></ul></nav></div></details></div><div class=post-content><p>在处理大规模数据时，数据无法全部载入内存，我们通常用两个选项</p><ul><li>使用<code>tfrecords</code></li><li>使用 <code>tf.data.Dataset.from_generator()</code></li></ul><p><a href=https://www.cnblogs.com/gongyanzh/p/16266794.html>tfrecords的并行化使用</a>前文已经有过介绍，这里不再赘述。如果我们不想生成tfrecord中间文件，那么生成器就是你所需要的。</p><p>本文主要记录针对 <code>from_generator()</code>的并行化方法，在 <code>tf.data</code> 中，并行化主要通过 <code>map</code>和 <code>num_parallel_calls</code> 实现，但是对一些场景，我们的<code>generator()</code>中有一些处理逻辑，是无法直接并行化的，最简单的方法就是将<code>generator()</code>中的逻辑抽出来，使用<code>map</code>实现。</p><h1 id=tfdatadataset-generator-并行>tf.data.Dataset generator 并行<a hidden class=anchor aria-hidden=true href=#tfdatadataset-generator-并行>#</a></h1><p>对<code>generator()</code>中的复杂逻辑，我们对其进行简化，即仅在生成器中做一些下标取值的类型操作，将<code>generator()</code>中处理部分使用<code>py_function</code> 包裹(wrapped) ，然后调用map处理。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>func</span><span class=p>(</span><span class=n>i</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>i</span> <span class=o>=</span> <span class=n>i</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span> <span class=c1># Decoding from the EagerTensor object</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>your_processing_function</span><span class=p>(</span><span class=n>training_set</span><span class=p>[</span><span class=n>i</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>x</span><span class=p>,</span> <span class=n>y</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>z</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>training_set</span><span class=p>)))</span> <span class=c1># The index generator</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>dataset</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>Dataset</span><span class=o>.</span><span class=n>from_generator</span><span class=p>(</span><span class=k>lambda</span><span class=p>:</span> <span class=n>z</span><span class=p>,</span> <span class=n>tf</span><span class=o>.</span><span class=n>uint8</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>dataset</span> <span class=o>=</span> <span class=n>dataset</span><span class=o>.</span><span class=n>map</span><span class=p>(</span><span class=k>lambda</span> <span class=n>i</span><span class=p>:</span> <span class=n>tf</span><span class=o>.</span><span class=n>py_function</span><span class=p>(</span><span class=n>func</span><span class=o>=</span><span class=n>func</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                                               <span class=n>inp</span><span class=o>=</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> 
</span></span><span class=line><span class=cl>                                               <span class=n>Tout</span><span class=o>=</span><span class=p>[</span><span class=n>tf</span><span class=o>.</span><span class=n>uint8</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                                     <span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>]</span>
</span></span><span class=line><span class=cl>                                               <span class=p>),</span> 
</span></span><span class=line><span class=cl>                      <span class=n>num_parallel_calls</span><span class=o>=</span><span class=n>tf</span><span class=o>.</span><span class=n>data</span><span class=o>.</span><span class=n>AUTOTUNE</span><span class=p>)</span>
</span></span></code></pre></div><p>由于隐式推断的原因，有时tensor的输出shape是未知的，需要额外处理</p><blockquote><p>A Tensor&rsquo;s shape (that is, the rank of the Tensor and the size of each dimension) may not always be fully known. In <a href=https://www.tensorflow.org/api_docs/python/tf/function><code>tf.function</code></a> definitions, the shape may only be partially known.</p><p>Most operations produce tensors of fully-known shapes if the shapes of their inputs are also fully known, but in some cases it&rsquo;s only possible to find the shape of a tensor at execution time.</p></blockquote><pre tabindex=0><code>dataset = dataset.batch(8)
def _fixup_shape(x, y):
    x.set_shape([None, None, None, nb_channels]) # n, h, w, c
    y.set_shape([None, nb_classes]) # n, nb_classes
    return x, y
dataset = dataset.map(_fixup_shape)
</code></pre><h1 id=tftensor与tfeagertensor>tf.Tensor与tf.EagerTensor<a hidden class=anchor aria-hidden=true href=#tftensor与tfeagertensor>#</a></h1><p><strong>为什么需要 <code>tf.py_function</code>，先来看下<code>tf.Tensor</code>与<code>tf.EagerTensor</code></strong></p><p>EagerTensor是实时的，可以在任何时候获取到它的值，即通过numpy获取</p><p>Tensor是非实时的，它是静态图中的组件，只有当喂入数据、运算完成才能获得该Tensor的值，</p><blockquote><p>map中映射的函数运算，而仅仅是告诉dataset，你每一次拿出来的样本时要先进行一遍function运算之后才使用的，所以function的调用是在每次迭代dataset的时候才调用的，属于<strong>静态图逻辑</strong></p></blockquote><pre tabindex=0><code>tensorflow.python.framework.ops.EagerTensor
tensorflow.python.framework.ops.Tensor
</code></pre><p><strong><code>tf.py_function</code>在这里起了什么作用？</strong></p><blockquote><p>Wraps a python function into a TensorFlow op that executes it eagerly.</p></blockquote><p>刚才说到map数据静态图逻辑，默认参数都是Tensor。而 使用<code>tf.py_function()</code>包装后，参数就变成了EagerTensor。</p><h3 id=references>references<a hidden class=anchor aria-hidden=true href=#references>#</a></h3><p>【1】https://medium.com/@acordier/tf-data-dataset-generators-with-parallelization-the-easy-way-b5c5f7d2a18</p><p>【2】https://blog.csdn.net/qq_27825451/article/details/105247211</p><p>【3】https://www.tensorflow.org/guide/data_performance#parallelizing_data_extraction</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://aida-yy.github.io/tags/tensorflow/>tensorflow</a></li></ul><nav class=paginav><a class=prev href=https://aida-yy.github.io/posts/gnn/><span class=title>« Prev</span><br><span>GNN</span></a>
<a class=next href=https://aida-yy.github.io/posts/%E6%A8%A1%E5%9E%8B%E5%8F%AC%E5%9B%9E%E4%B9%8Bdssm/><span class=title>Next »</span><br><span>模型召回之DSSM</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share 并行化tf数据生成器 on twitter" href="https://twitter.com/intent/tweet/?text=%e5%b9%b6%e8%a1%8c%e5%8c%96tf%e6%95%b0%e6%8d%ae%e7%94%9f%e6%88%90%e5%99%a8&url=https%3a%2f%2faida-yy.github.io%2fposts%2f%25E5%25B9%25B6%25E8%25A1%258C%25E5%258C%2596tf%25E6%2595%25B0%25E6%258D%25AE%25E7%2594%259F%25E6%2588%2590%25E5%2599%25A8%2f&hashtags=tensorflow"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 并行化tf数据生成器 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2faida-yy.github.io%2fposts%2f%25E5%25B9%25B6%25E8%25A1%258C%25E5%258C%2596tf%25E6%2595%25B0%25E6%258D%25AE%25E7%2594%259F%25E6%2588%2590%25E5%2599%25A8%2f&title=%e5%b9%b6%e8%a1%8c%e5%8c%96tf%e6%95%b0%e6%8d%ae%e7%94%9f%e6%88%90%e5%99%a8&summary=%e5%b9%b6%e8%a1%8c%e5%8c%96tf%e6%95%b0%e6%8d%ae%e7%94%9f%e6%88%90%e5%99%a8&source=https%3a%2f%2faida-yy.github.io%2fposts%2f%25E5%25B9%25B6%25E8%25A1%258C%25E5%258C%2596tf%25E6%2595%25B0%25E6%258D%25AE%25E7%2594%259F%25E6%2588%2590%25E5%2599%25A8%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 并行化tf数据生成器 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2faida-yy.github.io%2fposts%2f%25E5%25B9%25B6%25E8%25A1%258C%25E5%258C%2596tf%25E6%2595%25B0%25E6%258D%25AE%25E7%2594%259F%25E6%2588%2590%25E5%2599%25A8%2f&title=%e5%b9%b6%e8%a1%8c%e5%8c%96tf%e6%95%b0%e6%8d%ae%e7%94%9f%e6%88%90%e5%99%a8"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 并行化tf数据生成器 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2faida-yy.github.io%2fposts%2f%25E5%25B9%25B6%25E8%25A1%258C%25E5%258C%2596tf%25E6%2595%25B0%25E6%258D%25AE%25E7%2594%259F%25E6%2588%2590%25E5%2599%25A8%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 并行化tf数据生成器 on whatsapp" href="https://api.whatsapp.com/send?text=%e5%b9%b6%e8%a1%8c%e5%8c%96tf%e6%95%b0%e6%8d%ae%e7%94%9f%e6%88%90%e5%99%a8%20-%20https%3a%2f%2faida-yy.github.io%2fposts%2f%25E5%25B9%25B6%25E8%25A1%258C%25E5%258C%2596tf%25E6%2595%25B0%25E6%258D%25AE%25E7%2594%259F%25E6%2588%2590%25E5%2599%25A8%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share 并行化tf数据生成器 on telegram" href="https://telegram.me/share/url?text=%e5%b9%b6%e8%a1%8c%e5%8c%96tf%e6%95%b0%e6%8d%ae%e7%94%9f%e6%88%90%e5%99%a8&url=https%3a%2f%2faida-yy.github.io%2fposts%2f%25E5%25B9%25B6%25E8%25A1%258C%25E5%258C%2596tf%25E6%2595%25B0%25E6%258D%25AE%25E7%2594%259F%25E6%2588%2590%25E5%2599%25A8%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://aida-yy.github.io/>Aida’s home</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>